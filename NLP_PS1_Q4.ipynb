{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Processing: Assignment 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain, accumulate, tee\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus.reader.util import ConcatenatedCorpusView, StreamBackedCorpusView\n",
    "from nltk.corpus.europarl_raw import english as europarl_en\n",
    "\n",
    "random.seed(93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentence Loading</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_read_sent_block(stream):\n",
    "    \n",
    "    sentence = stream.readline().rstrip()\n",
    "    if len(sentence):\n",
    "        return [[word.lower() for word in sentence.split() if word.isalnum()]]\n",
    "    \n",
    "    return []\n",
    "\n",
    "if hasattr(europarl_en, \"_LazyCorpusLoader__load()\"):\n",
    "    europarl_en._LazyCorpusLoader__load()\n",
    "europarl_en._read_sent_block = custom_read_sent_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sents = list(europarl_en.sents())\n",
    "random.shuffle(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Corpus Splitting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpus_sents_(corpus_sents, factors = (0.08, 0.1, 0.1)):\n",
    "    \n",
    "    start_idx   = 0\n",
    "    split_sents = []\n",
    "    \n",
    "    for end_idx in map(lambda x: int(x * len(corpus_sents)), accumulate((1.0 - sum(factors),) + factors)):\n",
    "        split_sents.append(corpus_sents[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "        \n",
    "    return split_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_factors = (0.08, 0.1, 0.1)\n",
    "\n",
    "training_sents, validation_sents, development_sents, test_sents = split_corpus_sents_(corpus_sents, split_factors)\n",
    "\n",
    "num_training_sents = len(training_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Normalization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_(sents, hapaxes_offset = 1):\n",
    "    \n",
    "    fdist = nltk.FreqDist(nltk.ngrams(chain.from_iterable(sents), n=1))\n",
    "    \n",
    "    hapaxes = set(token for token, frequency in fdist.items() if frequency < hapaxes_offset)\n",
    "    num_unk = 0\n",
    "    for token in hapaxes:\n",
    "        num_unk += fdist.pop(token)\n",
    "    fdist[\"*unk*\"] = num_unk\n",
    "    \n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapaxes_offset = 10\n",
    "\n",
    "fdists = [create_vocab_(training_sents, hapaxes_offset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>N-Gram Calculation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_ngrams_(sent, vocab=None, n=1):\n",
    "    \n",
    "    return nltk.ngrams(chain(\n",
    "        [\"*start*\"] * (n - 1),\n",
    "         sent if vocab is None else map(lambda token: token if (token,) in vocab else \"*unk*\", sent),\n",
    "        [\"*end*\"]\n",
    "    ), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_dimensions = 3\n",
    "\n",
    "fdists.extend(map(\n",
    "    lambda sents, n: nltk.FreqDist(chain.from_iterable(map(partial(sent_ngrams_, vocab=fdists[0], n=n), sents))),\n",
    "    tee(training_sents, ngram_dimensions - 1),\n",
    "    range(2, ngram_dimensions + 1)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram_estimates_(sents, fdists, vocab, ngram_dimension=2):\n",
    "    \n",
    "    num_vocab = len(vocab)\n",
    "    sent_ngrams_fn_ = partial(sent_ngrams_, vocab=vocab, n=ngram_dimension)\n",
    "    estimates = {}\n",
    "    \n",
    "    for sent in sents:\n",
    "        \n",
    "        sent_ngrams = sent_ngrams_fn_(sent)\n",
    "        cumulative_nom_log = cumulative_denom_log = 0\n",
    "        \n",
    "        for ngram in sent_ngrams:\n",
    "            cumulative_nom_log   += np.log2(fdists[ngram_dimension - 1][ngram] + 1)\n",
    "            cumulative_denom_log += np.log2(fdists[ngram_dimension - 2][ngram[:-1]] + num_vocab)\n",
    "            \n",
    "        estimates[\" \".join(sent)] = cumulative_nom_log - cumulative_denom_log\n",
    "        \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_sents_(real_sents, vocab):\n",
    "    \n",
    "    vocab_tokens = [token[0] for token in vocab.keys()]\n",
    "    random_sents = [random.choices(vocab_tokens, k=len(sent)) for sent in real_sents]\n",
    "\n",
    "    return random_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_(sents, fdists, vocab, estimator_fn, ngram_dimension=2, model_name=\"Model\"):\n",
    "\n",
    "    true_estimates = estimator_fn(sents)\n",
    "    false_estimates = estimator_fn(make_random_sents_(sents, vocab))\n",
    "    \n",
    "    np_true_estimates  = np.fromiter(true_estimates.values(), dtype=np.float)\n",
    "    np_false_estimates = np.fromiter(false_estimates.values(), dtype=np.float)\n",
    "    \n",
    "    crossentropy = -np_true_estimates.sum() / (sum(map(len, sents)) + len(sents) * (ngram_dimension - 1))\n",
    "    \n",
    "    print(\"Language Model: {} | Sentences Average LogProb(STD): {}({}) | Random Sentences Average LogProb(STD): {}({}) | Crossentropy: {} | Perplexity: {}\".format(\n",
    "        model_name, np_true_estimates.mean(), np_true_estimates.std(), np_false_estimates.mean(), np_false_estimates.std(), crossentropy, 2 ** crossentropy\n",
    "    ), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: 2-Gram | Sentences Average LogProb(STD): -198.0404684299962(127.69854734026177) | Random Sentences Average LogProb(STD): -300.2155406600317(196.27538379748566) | Crossentropy: 7.575632716676362 | Perplexity: 190.7623587795095\n",
      "Language Model: 3-Gram | Sentences Average LogProb(STD): -241.1462377217555(154.62337506466056) | Random Sentences Average LogProb(STD): -300.9694897752996(195.72701371308523) | Crossentropy: 8.882389782851902 | Perplexity: 471.917147216093\n"
     ]
    }
   ],
   "source": [
    "for fdist, n in zip(fdists, range(1, ngram_dimensions)):\n",
    "    fdist[(\"*start*\",) * n] = num_training_sents\n",
    "\n",
    "ngram_estimator_fns = {n: partial(make_ngram_estimates_, fdists=fdists, vocab=fdists[0], ngram_dimension=n) for n in range(2, ngram_dimensions + 1)}\n",
    "    \n",
    "for n in range(2, ngram_dimensions + 1):\n",
    "    evaluate_model_(training_sents, fdists, vocab=fdists[0], estimator_fn=ngram_estimator_fns[n], ngram_dimension=n, model_name=\"{}-Gram\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Hyper-Parameterization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: 2-Gram | Sentences Average LogProb(STD): -208.1427119957932(131.2019715604889) | Random Sentences Average LogProb(STD): -308.37189942107835(188.3389820182272) | Crossentropy: 7.79290707950603 | Perplexity: 221.7679512417197\n",
      "Language Model: 3-Gram | Sentences Average LogProb(STD): -262.68157182482884(165.54647879279685) | Random Sentences Average LogProb(STD): -308.98458638330595(187.91481652326283) | Crossentropy: 9.479277306330548 | Perplexity: 713.7511260547658\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, ngram_dimensions + 1):\n",
    "    evaluate_model_(validation_sents, fdists, estimator_fn=ngram_estimator_fns[n], vocab=fdists[0], ngram_dimension=n, model_name=\"{}-Gram\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: 2-Gram | Sentences Average LogProb(STD): -202.6171279498383(118.89002182250977) | Random Sentences Average LogProb(STD): -299.6441424922481(172.12018526962413) | Crossentropy: 7.803999303926824 | Perplexity: 223.47959550552298\n",
      "Language Model: 3-Gram | Sentences Average LogProb(STD): -255.44767958017556(150.08991616031932) | Random Sentences Average LogProb(STD): -300.36950480624574(171.55141861822946) | Crossentropy: 9.473215163412483 | Perplexity: 710.7582666576358\n"
     ]
    }
   ],
   "source": [
    "for n in range(2, ngram_dimensions + 1):\n",
    "    evaluate_model_(development_sents, fdists, estimator_fn=ngram_estimator_fns[n], vocab=fdists[0], ngram_dimension=n, model_name=\"{}-Gram\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Interpolation Model Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interpolation_estimates_(sents, fdists, vocab, l=(1 / 3,) * 2):\n",
    "    \n",
    "    l = (1 - sum(l),) + l\n",
    "    assert l[0] >= 0\n",
    "    num_vocab = len(vocab)\n",
    "    sent_ngrams_fn_ = partial(sent_ngrams_, vocab=vocab, n=len(fdists))\n",
    "    estimates = {}\n",
    "    \n",
    "    for sent in sents:\n",
    "        \n",
    "        sent_ngrams = sent_ngrams_fn_(sent)\n",
    "        sent_prop_log = 0\n",
    "        \n",
    "        for ngram in sent_ngrams:\n",
    "            interpol_sum = l[0] * fdists[0].freq(ngram[:-1])\n",
    "            for n in range(2, len(fdists) + 1):\n",
    "                interpol_sum  += l[n - 1] * (fdists[n - 1][ngram[-n:]] + 1) / (fdists[n - 2][ngram[-n:-1]] + num_vocab)\n",
    "            sent_prop_log += np.log2(interpol_sum)\n",
    "            \n",
    "        estimates[\" \".join(sent)] = sent_prop_log\n",
    "        \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "\n",
      "Language Model: Interpolated 3-Gram | Sentences Average LogProb(STD): -203.12852404611402(130.76272578584627) | Random Sentences Average LogProb(STD): -302.106310429056(197.3296582135732) | Crossentropy: 7.482043857034284 | Perplexity: 178.78028688303735\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test Set Evaluation:\n",
      "\n",
      "Language Model: Interpolated 3-Gram | Sentences Average LogProb(STD): -208.37304546918386(122.09237477754071) | Random Sentences Average LogProb(STD): -301.5565779204233(173.04727390691872) | Crossentropy: 7.727463789177062 | Perplexity: 211.93290491870502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interpolation_params = (3 / 4,  1 / 5)\n",
    "interpolation_estimator_fn = partial(make_interpolation_estimates_, fdists=fdists, vocab=fdists[0], l=interpolation_params)\n",
    "\n",
    "print(\"Training Set Evaluation:\", end=\"\\n\\n\")\n",
    "evaluate_model_(training_sents, fdists, vocab=fdists[0], estimator_fn=interpolation_estimator_fn, ngram_dimension=len(fdists), model_name=\"Interpolated {}-Gram\".format(len(fdists)))\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"Test Set Evaluation:\", end=\"\\n\\n\")\n",
    "evaluate_model_(development_sents, fdists, vocab=fdists[0], estimator_fn=interpolation_estimator_fn, ngram_dimension=len(fdists), model_name=\"Interpolated {}-Gram\".format(len(fdists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
